---
title: "Advertising Analysis"
author: "Mercy M. Kubania"
date: "7/18/2020"
output: html_document
---

## Defining the Question

A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads. 

### a)Specifying the Question

1.Build a model that can predict if an Individual will  click on the client's Ads

### b)The metric of Success
Build a model that will best predict if a customer will click on Ad or not, with an accuracy of above 85%

### c)The Context
Online marketing for a Cryptography Class by using Ads

### d)Experimental Design
1. Load Data
2. Check the Data
3. Tidy the Data
4. Analyze the Data


## Reading the Data
```{r }
# plotting libraries
library("dplyr")
library("ggplot2")

```
```{r }
# Load the dataset
advertising <- read.csv("~/Documents/R markdowns/advertising.csv")
head(advertising)

```
## Checking the Data
```{r }
# The shape of the dataset
dim(advertising)
```

```{r }
# The Structure of the Data 
str(advertising)
```


## Tidying the Data
```{r }
#Missing Data
colSums(is.na(advertising))
# No missing values
```

```{r }
#Duplicated Values 
advert <- unique(advertising)
dim(advert)
# No duplicated values
```


```{r }
#Outliers

boxplot(advert$Daily.Time.Spent.on.Site,xlab="Daily Time Spent on Site",main="Boxplot on Daily Time Spent on Site")
boxplot(advert$Age, xlab="Age",main="Boxplot on Age")
boxplot(advert$Area.Income,xlab="Area Income",main="Boxplot on Area Income")
boxplot(advert$Daily.Internet.Usage, xlab="Daily Internet Usage", main="Boxplot of Daily Internet Usage")
boxplot(advert$Male, xlab="Male", main="Boxplot of Male")
boxplot(advert$Clicked.on.Ad, xlab="Clicked on Ad", main="Boxplot of Clicked on Ad")

# The number of outliers
boxplot.stats(advert$Area.Income)$out
sum(table(boxplot.stats(advert$Area.Income)$out))

```
The dataset has no missing values or Duplicated values. From the boxplots we see that the Area Income is the only numeric column with outliers, there are 8 columns

Outliers were not removed to see their role in the analysis

## Exploratory Data Analysis

### Univariate
#### Numeric Columns
```{r Summary}
#Numeric Values
num = advert[,c(1,2,3,4,7,10)]
summary(num)
```

```{r Mean}
#Mean
print(paste("Mean for Daily Time Spent on Site:",mean(advert$Daily.Time.Spent.on.Site)))
print(paste("Mean for Age:",mean(advert$Age)))
print(paste("Mean for Area Income:",mean(advert$Area.Income)))
print(paste("Mean for Daily Internet Usage:",mean(advert$Daily.Internet.Usage)))
print(paste("Mean for Male:", mean(advert$Male)))
print(paste("Mean for Cliked on Ad:",mean(advert$Male)))

```

```{r Median }
# Median
print("Median for Daily Time Spent on Site:")
median(advert$Daily.Time.Spent.on.Site)
print("Median for Age:")
median(advert$Age)
print("Median for Area Income:")
median(advert$Area.Income)
print("Median for Daily Internet Usage:")
median(advert$Daily.Internet.Usage)
print("Median for Male:")
median(advert$Male)
print("Median for Cliked on Ad:")
median(advert$Clicked.on.Ad)
```

```{r Mode }
# Mode
getMode <- function(v){
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
print("Mode for Daily Time Spent on Site:")
getMode(advert$Daily.Time.Spent.on.Site)
print("Mode for Age:")
getMode(advert$Age)
print("Mode for Area Income:")
getMode(advert$Area.Income)
print("Mode for Daily Internet Usage:")
getMode(advert$Daily.Internet.Usage)
print("Mode for Male:")
getMode(advert$Male)
print("Mode for Clicked on Ad")
getMode(advert$Clicked.on.Ad)
```

```{r Variance }
#Variance
print("Variance for Daily Time Spent on Site:")
var(advert$Daily.Time.Spent.on.Site)
print("Variance for Age:")
var(advert$Age)
print("Variance for Area Income:")
var(advert$Area.Income)
print("Variance for Daily Internet Usage:")
var(advert$Daily.Internet.Usage)
print("Variance for Male:")
var(advert$Male)
print("Variance for Clicked on Ad:")
var(advert$Clicked.on.Ad)

```

```{r Standard Deviation}

# Standard Deviation
print("Standard Deviation for Daily Time Spent on Site:")
sd(advert$Daily.Time.Spent.on.Site)
print("Standard Deviation for Age:")
sd(advert$Age)
print("Standard Deviation for Area Income:")
sd(advert$Area.Income)
print("Standard Deviation for Daily Internet Usage:")
sd(advert$Daily.Internet.Usage)
print("Standard Deviation for Male:")
sd(advert$Male)
print("Standard Deviation for Clicked on Ad:")
sd(advert$Clicked.on.Ad)
```

```{r Quantiles }
# Quantiles
print("Quantiles for Daily Time Spent on Site:")
quantile(advert$Daily.Time.Spent.on.Site)
print("Quantiles for Age:")
quantile(advert$Age)
print("Quantiles for Area Income:")
quantile(advert$Area.Income)
print("Quantiles for Daily Internet Usage:")
quantile(advert$Daily.Internet.Usage)
print("Quantiles for Male:")
quantile(advert$Male)
print("Quantiles for Clicked on Ad:")
quantile(advert$Clicked.on.Ad)
```

#### Histogram
```{r histogram, echo=FALSE}
hist(advert$Daily.Time.Spent.on.Site, breaks = 20,main ="Daily Time Spent on Site", col = "dodgerblue")
hist(advert$Age, breaks = 10,main = "Age",col = "dodgerblue")
hist(advert$Area.Income, breaks = 20, main = "Area Income",col = "dodgerblue")
hist(advert$Daily.Internet.Usage,breaks = 20,main = "Daily Internet Usage",col = "dodgerblue")
hist(advert$Male,breaks = 10,main = "Male",col = "dodgerblue")
hist(advert$Clicked.on.Ad,breaks = 10,main = "Clicked on Ad",col = "dodgerblue")
```
From the Male and the Clicked on Ad are categorical in nature

The distribution of Age  is skewed to the left, the Area Income and Daily Time Spent on Site distribution is skewed to the right

The Most frequent age is between approximately 25years and 40 years


#### BarPlots
```{r Barplots, echo=FALSE}
# How many Males Vs. Females
male <- table(advert$Male)
barplot(male,main = "Male",col = topo.colors(2),ylim = c(0, 800))
legend("topright",inset = .02, title="Gender",
       c("Female","Male"), fill=topo.colors(2), cex=0.8)

# No. of Clicked vs. Not Clicked
clicked <- table(advert$Clicked.on.Ad)
barplot(clicked,main = "Clicked on Ad",col = topo.colors(2), ylim = c(0,800))
legend("topright",inset = .02, title="Clicked on Ad",
       c("Not Clicked","Clicked"), fill=topo.colors(2), cex=0.8)

# the most popular countries
par(las=2, cex.axis=0.7)
country <- table(advert$Country)
barplot(sort(country[1:40], decreasing = TRUE), main = "Country",col = terrain.colors(20))

# The most popular Age
par(las=2)
age <- table(advert$Age)
barplot(sort(age[1:20], decreasing = TRUE), main = "Age",col = terrain.colors(20))

# group by gender/Male
# Gender that Spends more Time on the Internet
by_time <- advert %>% 
  group_by(Male) %>% 
  summarise(Total.Time.Spent.on.Site = sum(Daily.Time.Spent.on.Site))
p <- ggplot(by_time, aes(x = factor(Male), y = Total.Time.Spent.on.Site, fill = factor(Male)))+geom_bar(stat="identity")
p + scale_fill_discrete(name = "Male", labels = c("Female","Male"))+ labs(title="Gender that spends more time on the Internet", x="Gender")


# Group by country, Country with more clicks on Ad
by_country <- advert %>% group_by(Country) %>% summarise(clicked.ad =sum(Clicked.on.Ad[Clicked.on.Ad == 1]))

# select the first 20
rows <- by_country[1:20,]
rows

c <- ggplot(rows, aes(x = reorder(Country,clicked.ad), y=clicked.ad)) + geom_col() +coord_flip() +  geom_bar(stat="identity", fill="dodgerblue")
c + labs(title="Country with Highest Clicks on Ads", x="Countries", y="Clicked Ads")

# Females that click on ads
by_gender <- advert %>% group_by(Clicked.on.Ad) %>% summarise(gender = length(Male[Male == 0]))
by_gender

females <- ggplot(by_gender, aes(x = factor(Clicked.on.Ad), y = gender, fill=factor(Clicked.on.Ad))) + geom_bar(stat="identity")
females + scale_fill_discrete(name = "Ad Clicked", labels = c("Not Clicked","Clicked"))+ labs(title="Females that Clicked vs Not Clicked", x="Clicked on Ad", y="No. of Females")

# Males that clicked on ads

by_males <- advert %>% group_by(Clicked.on.Ad) %>% summarise(gender = length(Male[Male == 1]))
by_males

males <- ggplot(by_males, aes(x = factor(Clicked.on.Ad), y = gender, fill=factor(Clicked.on.Ad))) + geom_bar(stat="identity")
males + scale_fill_discrete(name = "Ad Clicked", labels = c("Not Clicked","Clicked"))+ labs(title="Males that Clicked vs Not Clicked", x="Clicked on Ad", y="No. of Males")

```
```{r }
# Ad topic Line
value.count <- table(advert$Ad.Topic.Line)
head(value.count)
sum(value.count)

# Only one occurrence of every topic line
```
From the Male column, 0 means Female and 1 means Male, we have more Females than Males
In the Clicked on Ad column, number of clicked versus not clicked are equal

The most popular countries are Afghanistan, Australia, Albania, Bahamas are the top 4 countries
The most popular age is 31 years
Females spend more time in the Internet compared to Males and with more Females spending time on the Internet, we see that most of them click on Ads compared to Males

But there is still a significant number of Females who don't click on Ads.

The Countries that had the highest number in clicked ads was Australia, Afghanistan and Bahamas

The Ad Topic Line there is only one occurrence of each.

### Bivariate

#### Covariance
```{r covariance}
numeric_col <- advert[,c(1,2,3,4,7,10)]
head(numeric_col,4)

# Covariance
covariance_matrix = cov(numeric_col)
#View(round(covariance_matrix,2))
covariance <-as.data.frame(round(covariance_matrix,2))

```

#### Correlation
```{r correlation}
# Correlation Matrix
correlation_matrix = cor(numeric_col)
#View(round(correlation_matrix,2))
corr <- as.data.frame(round(correlation_matrix,2))
corr
```
Clicked on Ad column has strong negative correlation with Daily Time Spent on Internet and Daily Internet Usage
It has a moderate negative Correlation with Area Income and a Moderate Positive Correlation with Age

Therefore this columns
1.Daily Time Spent on Internet
2.Daily Internet Usage
3.Area Income
4.Age
Can we used to determine if a an Ad will be clicked on or not

#### Scatter Plots

```{r scatter plot, echo=FALSE}
area.income <- advert$Area.Income
internet.usage <- advert$Daily.Internet.Usage
time.spent <- advert$Daily.Time.Spent.on.Site

plot(area.income, internet.usage, xlab="Area Income",ylab = "Daily Internet Usage",main = "Area Income vs Daily Internet Usage")
plot(area.income,time.spent,xlab = "Area Income",ylab = "Daily Time Spent on the Internet",main="Area Income vs Daily Time Spent on the Internet")
plot(time.spent,internet.usage, xlab="Daily Time spent", ylab="Daily Internet Usage",main = "Daily Time Spent on Site vs Daily Internet Usage")

```

The more the Area Income the more time the user spent on the Internet and also the the more internet used

##  Implementing the Solution

```{r Libraries}
# Data Analysis
library("caret")
library("tidyverse")

# SVM and Naive Bayes library
library("e1071")

# plotting libraries
library("rpart")
library("rpart.plot")

# Decision Trees library
library("ISLR")

```

### K-Nearest Neighbours
```{r Normalize }
# Normalize our features
features <- advert[,c(1,2,3,4,7)]

# The normalization function is created
normalize <-function(x) { (x -min(x))/(max(x)-min(x))}

# Normalization function is applied to the dataframe
advert_norm <- as.data.frame(lapply(features, normalize))
head(advert_norm)

summary(advert_norm)

```

```{r Split the Data}
# Generate a random number that is 80% of the total number of rows in dataset
train <- sample(1:nrow(advert), 0.8 * nrow(advert)) 

#training data
advert_train <- advert_norm[train,]
advert_train_target <- as.factor(advert[train,10])
# testing data
advert_test <- advert_norm[-train,]
advert_test_target <- as.factor(advert[-train,10])

dim(advert_train)
dim(advert_test)
```

```{r KNN model}
# Applying k-NN classification algorithm.
library(class)
# No. of neighbors are generally square root of total number of instances
neigh <- round(sqrt(nrow(advert)))+1 

knn_model <- knn(advert_train,advert_test, cl=advert_train_target, k=neigh)

# Visualizing classification results
cm_knn <- confusionMatrix(table(advert_test_target, knn_model))
cm_knn
```

### Decision Trees
```{r}
anyNA(advert)
```

```{r Decision Trees}

# convert the target column to a factor
advert$Clicked.on.Ad <- as.factor(advert$Clicked.on.Ad)
features = advert[,c(1,2,3,4,7,10)]

# Split the Data into Train and Test into 80:20 split
intrain <- createDataPartition(y = advert$Clicked.on.Ad, p= 0.8, list = FALSE)
training <- features[intrain,]
testing <- features[-intrain,]

set.seed(42)
myGrid <- expand.grid(mtry = sqrt(ncol(advert)),
                     splitrule = c("gini", "extratrees"),
                     min.node.size = 20)

dt_model <- train(Clicked.on.Ad ~ .,
               data = training,
               method = "ranger", 
               tuneGrid = myGrid,
               trControl = trainControl(method='repeatedcv', 
                                        number=10, 
                                        repeats=3,
                                        search = 'random',
                                       verboseIter = FALSE))

dt_model

plot(dt_model)

# Make predictions and check accuracy 
dt_pred <- predict(dt_model,testing )
cm_dt <- confusionMatrix(table(dt_pred, testing$Clicked.on.Ad))
cm_dt
```
### Support Vector Machines
```{r SVM Linear}
# Split the Data into Train and Test into 80:20 split
intrain <- createDataPartition(y = advert$Clicked.on.Ad, p= 0.8, list = FALSE)
training <- features[intrain,]
testing <- features[-intrain,]

set.seed(42)
svm_Linear <- train(Clicked.on.Ad ~., data = training, method = "svmLinear",
trControl=trainControl(method = "repeatedcv", 
                       number = 10,
                       repeats = 3),
                      preProcess = c("center", "scale"))

# preProcess -> deals with normalization

svm_Linear

# Make predictions and check accuracy 
test_pred <- predict(svm_Linear, testing)
cm_svmlinear <- confusionMatrix(table(test_pred, testing$Clicked.on.Ad))
cm_svmlinear
```
```{r SVM Radial}

set.seed(42)
mygrid <- expand.grid(C = c(0.1,1,10,100,1000),
                      sigma = c(0.5,1,2,3,4))

svm_Radial <- train(Clicked.on.Ad ~., data = training, method = "svmRadial",
                    tuneGrid = mygrid,
trControl=trainControl(method = "repeatedcv", 
                       number = 10,
                       repeats = 3),
                      preProcess = c("center", "scale"))

svm_Radial
# Make predictions and check accuracy 
radial_test <- predict(svm_Radial, testing)
cm_svmradial <- confusionMatrix(table(radial_test, testing$Clicked.on.Ad))
cm_svmradial
```
### Naive Bayes
```{r Naive Bayes}
# split the training into Features and labels for the model

x = training[,1:4]
y = training$Clicked.on.Ad

nb_model <- train(x,y, "nb", trControl = trainControl(method = "repeatedcv", 
                       number = 10,
                       repeats = 3),
                      preProcess = c("range"))

# Make prediction
nb_pred <- predict(nb_model , testing)

# Accuracy
cm_nb <- confusionMatrix(table(nb_pred, testing$Clicked.on.Ad))
cm_nb

```

## Performance Comparison
```{r Tabulate Model Performance}
# create a function to compare model performances
model_compare <- data.frame(Model = c("KNN",
                                      "Random Forest",
                                      "SVM Linear",
                                      "SVM Radial",
                                      "Naive Bayes"),
                            Accuracy = c(cm_knn$overall[1],
                                         cm_dt$overall[1],
                                         cm_svmlinear$overall[1],
                                         cm_svmradial$overall[1],
                                         cm_nb$overall[1]))

model_compare
# plot the comparisons
ggplot(model_compare, aes(x = Model, y=Accuracy))+ geom_bar(stat = "identity", fill ="dodgerblue") + labs(title = "Comparing Models Accuracy", x="Models", y="Overall Accuracy" )

```


The Models have performed quite well with SVM and Naive Bayes recording the highest performance of 97%
Therefore we can use this 2 Models to find out if a customer is likely to click on an Ad or not 

## Challenge the Solution
The problem was a Classification problem, from the models built we see that they all perform quite well with accuracies being above 95%. Model optimization was applied to all the models therefore this could be the reason for such good accuracies we have gotten

There isn't a problem of imbalanced data.

We can use other classification models to see if the accuracies will be better. 

