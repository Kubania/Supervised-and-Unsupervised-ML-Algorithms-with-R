---
title: "Customer Behaviour"
author: "Mercy M. Kubania"
date: "7/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem Definition

Kira Plastinina  is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.

### a)Specifying the Question
1. Perform clustering stating insights drawn from your analysis and visualizations.

2. Provide comparisons between the approaches learned this week i.e. K-Means clustering vs Hierarchical clustering highlighting the strengths and limitations of each approach in the context of your analysis. 


### b)The metric of Success
Be able to understand the characteristics of Kira Plastinina Customers and Classify the into by using unsupervised machine learning algorithms 

### c)The Context
Study of online shoppers for specifically the Kira Plastinina brand

### d)Experimental Design
1. Data Sourcing
2. Check the Data
3. Perform Data Cleaning
4. Analyze the Data

## Data Sourcing
```{r Data Sourcing}
shoppers <- read.csv("~/Documents/R markdowns/IP Week 13/online_shoppers_intention.csv")

head(shoppers)
```
## Check the Data
```{r Structure}
# check the structure of Data
str(shoppers)
```
```{r Dimensions}
dim(shoppers)

```
```{r Summary}
summary(shoppers)
```
```{r}
# convert all negative values to NA

shoppers[shoppers<0] <- NA
```

```{r Convert to factor }
 # convert into a factor
shoppers$VisitorType <- factor(shoppers$VisitorType)
head(shoppers$VisitorType)

shoppers$Weekend <- factor(shoppers$Weekend)
head(shoppers$Weekend)

shoppers$Revenue <- factor(shoppers$Revenue)
head(shoppers$Revenue)

shoppers$Month <- factor(shoppers$Month)
head(shoppers$Month)
```

## Perform Data Cleaning
```{r Missing values}
#check for missing values
colSums(is.na(shoppers))

anyNA(shoppers)

#library("Amelia")
#missmap(shoppers)

# use MICE to predict missing values
library("mice")
mice_mod <- mice(shoppers[, c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates")], method='rf')
mice_complete <- complete(mice_mod)

shoppers$Administrative <- mice_complete$Administrative
shoppers$Administrative_Duration  <- mice_complete$Administrative_Duration
shoppers$Informational <- mice_complete$Informational
shoppers$Informational_Duration <- mice_complete$Informational_Duration
shoppers$ProductRelated <- mice_complete$ProductRelated
shoppers$ProductRelated_Duration <- mice_complete$ProductRelated_Duration
shoppers$BounceRates <- mice_complete$BounceRates
shoppers$ExitRates <- mice_complete$ExitRates
```
```{r}
# Check for any missing values
anyNA(shoppers)
```

```{r Duplicates}
shoppers <- unique(shoppers)
dim(shoppers)
```

```{r Outliers}
num_col <- shoppers[ ,c(1,2,3,4,5,6,7,8,9,10,12,13,14,15)]

outlier_detection = function(x){
  for(i in colnames(x)){
    boxplot(shoppers[[i]], xlab=i, main=paste0("Boxplot for ",i))
  }
}

outlier_detection(num_col)
```

```{r Dealing with Outliers}
# replace outliers with the 5th and 95th percentile
outlier_replace <- function(x){
   qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
   caps <- quantile(x, probs=c(.05, .95), na.rm = T)
   H <- 1.5 * IQR(x, na.rm = T)
   x[x < (qnt[1] - H)] <- caps[1]
   x[x > (qnt[2] + H)] <- caps[2]
   return(x)
}
shoppers$Administrative <- outlier_replace(shoppers$Administrative)
shoppers$Administrative_Duration <-outlier_replace(shoppers$Administrative_Duration)
shoppers$Informational <- outlier_replace(shoppers$Informational)
shoppers$Informational_Duration <- outlier_replace(shoppers$Informational_Duration )
shoppers$ProductRelated <- outlier_replace(shoppers$ProductRelated)
shoppers$ProductRelated_Duration <- outlier_replace(shoppers$ProductRelated_Duration)
shoppers$BounceRates <- outlier_replace(shoppers$BounceRates)
shoppers$ExitRates <- outlier_replace(shoppers$ExitRates)
shoppers$PageValues <- outlier_replace(shoppers$PageValues)
shoppers$SpecialDay <- outlier_replace(shoppers$SpecialDay)
shoppers$OperatingSystems <- outlier_replace(shoppers$OperatingSystems)
shoppers$Browser <- outlier_replace(shoppers$Browser)
shoppers$Region <- outlier_replace(shoppers$Region)
shoppers$TrafficType <- outlier_replace(shoppers$TrafficType)

# check to see if there are more outliers
outlier_detection(num_col)
```

```{r eval=FALSE, include=FALSE}
# replace the remaining outliers with NA then predict the values
outlier_na = function(x){
  col <- c(2,3,4,5,6,7,8,9,10,13,15)
  for (i in col){
     outlier <- boxplot.stats(x)$out
     shoppers[x %in% outlier, names(shoppers)[i]] <- NA 
  }
}


shoppers$Administrative_Duration <-outlier_na(shoppers$Administrative_Duration)
shoppers$Informational <- outlier_na(shoppers$Informational)
shoppers$Informational_Duration <- outlier_na(shoppers$Informational_Duration )
shoppers$ProductRelated <- outlier_na(shoppers$ProductRelated)
shoppers$ProductRelated_Duration <- outlier_na(shoppers$ProductRelated_Duration)
shoppers$BounceRates <- outlier_na(shoppers$BounceRates)
shoppers$ExitRates <- outlier_na(shoppers$ExitRates)
shoppers$PageValues <- outlier_na(shoppers$PageValues)
shoppers$SpecialDay <- outlier_na(shoppers$SpecialDay)
shoppers$Browser <- outlier_na(shoppers$Browser)
shoppers$Region <- outlier_na(shoppers$Region)
shoppers$TrafficType <- outlier_na(shoppers$TrafficType)

colSums(is.na())
head(shoppers)
```
From our Analysis we see that we have a total of 12330 records with 18 columns, 10 columns are numeric and 8 columns are categorical

Columns that had discrete values and are categorical like:-
a) Months
b) Weekend
c) Revenue
d) Visitor Type

They were converted to factors and all the numeric columns with negative values were replace with NA.
The dataset had Missing values and they were replaced by the library MICE that predict the missing values using random forest.

There were duplicated values and there were dropped, reducing the records to 12210 records and 18 columns.

The numeric columns had outliers and capping was used by replacing the outliers with the 5th and 95th percentile, although some outliers remained.
The outliers were not removed.


## Perform Exploratory Data Analysis 

### Univariate
#### Categorical Variables
```{r Categorical Barplots, echo=FALSE}
library("ggplot2" )
# Group Special Days
specialday <- ggplot(shoppers, aes(x=factor(SpecialDay), fill = factor(SpecialDay))) + geom_bar()
specialday +  scale_fill_discrete(name = "Special Day", labels = c("Not Special","Special"))+ labs(title="Number of Special Days", x="Special Days")


# Count the Months
months <- ggplot(shoppers ,aes(x=Month , fill=factor(Month))) + geom_bar() + labs(title = "Distribution of the Months", x="Months")
months +scale_fill_discrete(name = "Month") 

# Count on Visitor Type
visitor <- ggplot(shoppers, aes(VisitorType, fill=factor(VisitorType)))+ geom_bar() + labs(title = "Distribution of Visitor Types", x="Special Days")
visitor + scale_fill_discrete(name = "Visitor Type") 

# Count on Revenue
revenue <- ggplot(shoppers, aes(Revenue, fill=factor(Revenue))) +geom_bar() + labs(title = "Distribution of Revenue")
revenue +scale_fill_discrete(name = "Revenue") 

# Group Revenue by Months
revenue1 <- ggplot(shoppers, aes(x=Month, fill= factor(Revenue)))+ geom_bar(position=position_dodge2(width = 1.2, preserve = "single"))
revenue1 + labs(title = "Distribution of Revenue in a Month") +scale_fill_discrete(name = "Revenue") 

# Group Special Days by Month
specialday1 <- ggplot(shoppers, aes(x=Month, fill= factor(SpecialDay)))+ geom_bar(position = position_dodge2(width = 1.2, preserve = "single"))
specialday1 + scale_fill_discrete(name = "Special Day", labels = c("Not Special","Special")) + labs(title = "Distribution of Special Days in a Month")

# Group Visitor Type by Month
visitor1 <- ggplot(shoppers, aes(x=Month, fill=factor(VisitorType)))+geom_bar(position=position_dodge2(width = 1.2, preserve = "single")) + labs(title = "Distribution of Visitor Type in a Month")
visitor1 + scale_fill_discrete(name = "Visitor Type") 

# Group Weekend by Month
weekend <- ggplot(shoppers, aes(x=Month, fill=factor(Weekend)))+geom_bar(position=position_dodge2(width = 1.2, preserve = "single"))+labs(title="Distribution of Revenue during weekends over the Months",subtitle = "(FALSE -No Revenue vs TRUE - Revenue)", x ="")+facet_wrap(~ factor(Revenue) )
weekend + scale_fill_discrete(name = "Weekend", labels = c("Not Weekend","Weekend")) 

# Distribution of Traffic Type
traffictype <- ggplot(shoppers, aes(x=factor(TrafficType), fill=factor(TrafficType)))+ geom_bar()+labs(title="Distribution of Traffic Type", x= "Traffic Type")

traffictype +scale_fill_discrete(name = "Traffic Type")

# Distribution of Browser
browser <-ggplot(shoppers, aes(x=factor(Browser), fill=factor(Browser)))+ geom_bar()+labs(title="Distribution of Browser", x = "Browser")
browser +scale_fill_discrete(name = "Browser")

# Distribution of Operating System
os <- ggplot(shoppers, aes(x=factor(OperatingSystems), fill= factor(OperatingSystems)))+ geom_bar()+labs(title="Distribution of  Operating Systems" , x="Operating Systems")
os +scale_fill_discrete(name = "Operating Systems") 

# Distribution of TRaffic Type in a Month
traffictype1 <- ggplot(shoppers, aes(x=Month, fill=factor(TrafficType))) +geom_bar(width = 0.8,position=position_dodge(width = 0.8))+labs(title="Distribution of Traffic Type in a Month")
traffictype1 + scale_fill_brewer(name ="Traffic Type",palette="Set1")

```
#### Numeric Variables
```{r Summary Stats}
library(Hmisc)
describe(num_col, skew=TRUE)
```

```{r Histograms}
histogram = function(x){
  for(i in colnames(x)){
    hist(shoppers[[i]], breaks = 10,main =i,xlab = i,col = "dodgerblue")
  }
}

histogram(num_col)

```
### Bivariate
#### Correlation
```{r Correlation}
# Convert the Revenue Col to Numeric
shoppers$Revenue <- as.numeric(shoppers$Revenue)
nums <- shoppers[ ,c(1,2,3,4,5,6,7,8,9,10,12,13,14,15,18)]

# Correlation matrix
corr_matrix = cor(nums)
corr <- as.data.frame(round(corr_matrix,2))
corr
```

```{r Scatter Plot}
admin <- shoppers$Administrative
admin_d <- shoppers$Administrative_Duration
info <- shoppers$Informational
info_d <- shoppers$Informational_Duration
prod <- shoppers$ProductRelated
prod_d <- shoppers$ProductRelated_Duration
exit <- shoppers$ExitRates
bounce <- shoppers$BounceRates
page <- shoppers$PageValues

# Administrative
plot(admin,exit,main = "Relationship between the Administrative and Exit Rate", xlab="Administrative", ylab="Exit Rate")
plot(admin,bounce,main = "Relationship between the Admintrative and Bounce Rate",xlab="Administrative",ylab = "Bounce Rate")
plot(admin,page, main = "Relationship between the Admintrative and Page Values",xlab="Administrative",ylab = "Page Value")
plot(admin, admin_d = "Relationship between the Administrative and Administrative Duration",xlab="Administrative",ylab = "Administrative Duration")

# Informational
plot(info,exit,main = "Relationship between the Informational and Exit Rate",xlab = "Informational",ylab = "Exit Rates")
plot(info,bounce,main = "Relationship between the Informational and Bounce Rate",xlab = "Informational", ylab = "Bounce Rates")
plot(info,page, main = "Relationship between the Informational and Page Values",xlab = "Informational",ylab = "Page Values")
plot(info, info_d, main = "Relationship between the Informational and Informational Duration", xlab = "Informational",ylab = "Informational Duration")


# Product Related
plot(prod,exit,main = "Relationship between the Product Related and Exit Rate",xlab="Product Related",ylab = "Exit Rates")
plot(prod,bounce,main = "Relationship between the Product Related and Bounce Rate",xlab="Product Related",ylab = "Bounce Rates")
plot(prod,page, main = "Relationship between the Product Related and Page Values",xlab="Product Related",ylab = "Page Values")
plot(prod, prod_d, main = "Relationship between the Product Related and ProductRelated Duration",xlab="Product Related",ylab ="Product Related Duration")

#Duration
plot(admin_d,info_d, main = "Relationship between Admin Duration and Informational Duration", xlab = "Administrative Duration", ylab = "Informational Duration")
plot(prod_d, info_d,main = "Relationship between ProductRelated Duration and Informational Duration", xlab = "Product Related Duration",ylab = "Informational Duration")
plot(admin_d,prod_d,main = "Relationship between Admin Duration and ProductRelated Duration", xlab = "Administrative Duration", ylab = "Product Related Duration")

```
```{r Categorical-Numeric, message=FALSE}
library(dplyr)
# how much page values do we have on weekends
by_weekend <- shoppers %>%
  group_by(Weekend) %>%
  summarise(pagevalues = sum(PageValues))

pv <- ggplot(by_weekend,aes(x=Weekend,y=pagevalues, fill=factor(Weekend)))+geom_bar(stat = "identity")+scale_fill_discrete(name = "Weekend", labels = c("Not Weekend","Weekend"))+ labs(title="Page Values do we have on Weekends", x="Weekend",y="Total value of Page Values")
pv 

# Pages with the Highest Exit rate
## Administrative
by_admin <- shoppers %>%
  group_by(Administrative)%>%
  summarise(Exit = sum(ExitRates))

ad <- ggplot(by_admin,aes(x=factor(Administrative),y=Exit, fill=factor(Administrative)))+geom_bar(stat = "identity")+ labs(title="Cumulative % of Exit Rate in Administratuve", x="Administrative Pages",y=" Cumulative % of Exit Rate per Page")
ad+theme(legend.position = "none")

## Informational
by_info <- shoppers %>%
  group_by(Informational)%>%
  summarise(Exit = sum(ExitRates))

inf <- ggplot(by_info,aes(x=factor(Informational),y=Exit, fill=factor(Informational)))+geom_bar(stat = "identity")+ labs(title="Cumulative % of Exit Rate in Informational", x="Informational Pages",y="Cumulative % of Exit Rateper Page")
inf+theme(legend.position = "none")

## Product Related
by_prod <- shoppers %>%
  group_by(ProductRelated)%>%
  summarise(Exit = sum(ExitRates))

pro <- ggplot(by_prod[1:20,],aes(x=reorder(factor(ProductRelated),Exit),y=Exit, fill=factor(ProductRelated)))+geom_bar(stat = "identity")+ labs(title="Cumulative % of Exit Rate in Product Related", x="Product Related Pages",y="Cumulative % of Exit Rate per Page")
pro+theme(legend.position = "none")

# Pages with the Highest Bounce rate
## Administrative
by_admin1 <- shoppers %>%
  group_by(Administrative)%>%
  summarise(Bounce = sum(BounceRates))

ad1 <- ggplot(by_admin1,aes(x=factor(Administrative),y=Bounce, fill=factor(Administrative)))+geom_bar(stat = "identity")+ labs(title="Cumulative % of  Bounce Rate in Administrative", x="Administrative Pages",y=" Cumulative % of Bounce Rate per Page")
ad1 +theme(legend.position = "none")

## Informational
by_info1 <- shoppers %>%
  group_by(Informational)%>%
  summarise(Bounce = sum(BounceRates))

inf1 <- ggplot(by_info1,aes(x=factor(Informational),y=Bounce, fill=factor(Informational)))+geom_bar(stat = "identity")+ labs(title="Cumulative % of  Bounce Rate in Informational", x="Informational Pages",y=" Cumulative % of Bounce Rate per Page")

inf1+theme(legend.position = "none")

## Product Related
by_prod1 <- shoppers %>%
  group_by(ProductRelated)%>%
  summarise(Bounce = sum(BounceRates))

pro1 <- ggplot(by_prod1[1:20,],aes(x=reorder(factor(ProductRelated),Bounce),y=Bounce, fill=factor(ProductRelated)))+geom_bar(stat = "identity")+ labs(title="Cumulative % of  Bounce Rate in Product Related",subtitle = "(Plotted for the Top 20 Pages)" ,x="Product Related Pages",y="Cumulative % Bounce Rate per Page")
pro1+theme(legend.position = "none")


# Pages with the Highest Bounce rate
## Administrative
by_admin2 <- shoppers %>%
  group_by(Administrative)%>%
  summarise(Page = sum(PageValues))

ad2 <- ggplot(by_admin2,aes(x=factor(Administrative),y=Page, fill=factor(Administrative)))+geom_bar(stat = "identity")+ labs(title="Total Average for Page Values in Administrative", x="Administrative Pages",y=" Total Average for Page Values per Page")
ad2 +theme(legend.position = "none")

## Informational
by_info2 <- shoppers %>%
  group_by(Informational)%>%
  summarise(Page = sum(PageValues))

inf2 <- ggplot(by_info2,aes(x=factor(Informational),y=Page, fill=factor(Informational)))+geom_bar(stat = "identity")+ labs(title="Total Average for Page Values in Informational", x="Informational Pages",y=" Total Average for Page Values per Page")

inf2+theme(legend.position = "none")

## Product Related
by_prod2 <- shoppers %>%
  group_by(ProductRelated)%>%
  summarise(Page = sum(PageValues))

pro2 <- ggplot(by_prod2[1:20,],aes(x=reorder(factor(ProductRelated),Page),y=Page, fill=factor(ProductRelated)))+geom_bar(stat = "identity")+ labs(title="Total Average forvPage Values in Product Related", x="Product Related Pages",y="Total Average for Page Values per Page")
pro2+theme(legend.position = "none")

# Exit Rates over the Months
exit_months <- shoppers %>%
  group_by(Month)%>%
  summarise(exit = sum(ExitRates), bounce =sum(BounceRates), pages = sum(PageValues))


#Exit Rate
mon <- ggplot(exit_months,aes(x=reorder(factor(Month),exit),y=exit, fill=factor(Month)))+geom_bar(stat = "identity")+ labs(title="Total Exit Rate Per Month", x="Months",y="Cumulative % for Exit Rate")

mon+theme(legend.position = "none")

#Bounce Rates
mon1 <- ggplot(exit_months,aes(x=reorder(factor(Month),bounce),y=bounce, fill=factor(Month)))+geom_bar(stat = "identity")+ labs(title="Total Bounce Rate Per Month", x="Months",y="Cumulative % for Bounce Rate")

mon1 + theme(legend.position = "none")

# PageValues
mon2 <- ggplot(exit_months,aes(x=reorder(factor(Month),pages),y=pages, fill=factor(Month)))+geom_bar(stat = "identity")+ labs(title="Total Average Page Values Per Month", x="Months",y="Total Average for Page Values")
mon2 + theme(legend.position = "none")

```

From our Analysis we observe a few interesting things that Special Days are in February and May.
The site is only active 10 out of 12 months of the year excluding January and April
Popular moths are:-
    1. May
    2. March
    3. November
    4. December

The months with most revenue are:-
    1. November
    2. May
    3. December

Although the site experiencies no revenue as compared to revenue, which is a case of imbalanced data that we will deal with when modelling

Interestingly February has special days yet it is not a popular month and also doesn't generate a lot of revenue

The visitors who come to the site are mostly Returning customers who are seen mostly in May,November, March and December.

In December and November we also observe a rare group of visitors, Others who are only seen during htis two months 
Also December,November and March experience highest numbers in new visitors
Therefore the Months:-
    1. December
    2. November
    3. March
    4. May

Should be the focus of the Sales and Marketing team, they are the months for most likely to retain its customers and also gain new customers

There is more activity during Weekdays compared to Weekends, most revenue being acquired on Weekdays compared to Weekends, but still there is significant activity with revenue in both Weekdays and Weekends

There are 3 Traffic Types the most popular is Type 2, it most active in November,March and May
In May three Traffic Types are most popular and that is Type 2,3 and 4. 
In March Type 1 is very active


Our Data is not Normally Distributed.
From our correlation table we see the Administrative and Administrative Duration, Informational and Informational Duration, and Product Related and Product Related Duration have strong positive correlation which are 0.77,0.94,0.85 respectively.

This means the more the User access the most hidden pages the more likely there are to stay on the site.

Revenue has a strong positive correlation with Page Values, a correlation of 0.6. An increase in Page Values the higher the revenue

Bounce Rates and Exit Rates also have a strong positive correlation of 0.78, where when a page has been visited it has a likely chance to "bounced" out on.
Therefore we see a lot of the pages that had high number of visitors also had a high number of Bounces. This were pages:-
     1. Page 0 - Administrative
     2. Page 0 - Informational
     3. Page 1,2,3,4 - Product Related
 
This are probably the first pages the user interacts with which might explain the lack of Revenue, therefore an area to focus on for the Sales and Marketing team although the Bounce Rates are less than Exit Rates the difference, however, is not that significant

The pages that are recording high values in Page Values are:-
    1.Page 0 and 9 - Administrative
    2. Page 0 - Informational
    3. Page 19,15,13 - Product Related

The products being viewed are in those pages you cannot easily locate

Even though the Months March, May, November and December have more Page Values and therefore Revenue, the Months December,March and November experience very high numbers of Bounce Rates.



## Implement the Solution
### K-Means Clustering
```{r Encoding}
# Transform Factors to Numeric
shoppers$Month <- as.numeric(shoppers$Month)
shoppers$VisitorType <- as.numeric(shoppers$VisitorType)
shoppers$Weekend <- as.numeric(shoppers$Weekend)

str(shoppers)
```
```{r Dealing with Imbalanced Data}
# K-Means is affected by Imbalanced data
table(shoppers$Revenue)
prop.table(table(shoppers$Revenue))

# our class label is imbalanced thus we shall balance it
# Split to train and test
library(ROSE)
library(caret)

# Make the Revenue Class a factor again
shoppers$Revenue <- as.factor(shoppers$Revenue)


# Use ROSE, it helps us to generate data synthetically as well
train.rose <- ROSE(Revenue ~ ., data = shoppers, seed = 1)$data
table(train.rose$Revenue)


feature <- train.rose[, -18]
label <- train.rose[,"Revenue"]

```

```{r Normalize}
# Normalize the data
normalize <- function(x){
  return ((x-min(x)) / (max(x)-min(x)))
}

# normalize the train
feat_norm <- as.data.frame(lapply(feature, normalize))


summary(feat_norm)

```
```{r Kmeans}
#Apply the model
model <- kmeans(feat_norm,2, nstart = 30)

# check for no. of records in each cluster
model$size

# a set of initial (distinct) cluster centers.
model$centers

# shows the cluster where each record falls
#model$cluster

#Verfy the results
plot(feat_norm[c(1:2)], col = model$cluster)
plot(feat_norm[c(1:2)], col = label)

plot(feat_norm[c(3:4)], col = model$cluster)
plot(feat_norm[c(3:4)], col = label)

plot(feat_norm[c(5:6)], col = model$cluster)
plot(feat_norm[c(5:6)], col = label)

```
```{r Accuracy }

tb <- table(model$cluster, label)
tb
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
paste0("Accuracy: ",accuracy(tb))

```
### Hierarchial Clustering
```{r Hclust}
# Scale our data first
data <- shoppers[,-18]

data <- scale(data)
head(data)

#calculate the Euclidean distance
d <- dist(data, method = "euclidean")

# apply the h-clustering and use different methods
model_comp <- hclust(d, method = "complete")

model_ward <- hclust(d, method = "ward.D")
model_ward2 <- hclust(d, method = "ward.D2")
model_sing <- hclust(d, method = "single")
model_avg <- hclust(d, method = "average")
model_mc <- hclust(d, method = "mcquitty")
model_med <- hclust(d, method = "median")
model_cent <- hclust(d , method = "centroid")

# Plot Dendograms

plot(model_comp, cex = 0.6, hang = -1)
plot(model_ward, cex = 0.6, hang = -1)
plot(model_ward2, cex = 0.6, hang = -1)
plot(model_sing, cex = 0.6, hang = -1)
plot(model_avg, cex = 0.6, hang = -1)
plot(model_mc, cex = 0.6, hang = -1)
plot(model_med, cex = 0.6, hang = -1)
plot(model_cent, cex = 0.6, hang =-1)
```
The two Clustering methods, K-Means Clustering achieved an accuracy of 39% while for the Hierarchical Clustering the model that uses the ward.D2 method best clusters. 
It forms clusters better than K-Means because we can see it forming a finally branch on the dendogram

The challenge with Hierarchical Clustering you cannot view the variables and how they formed the cluster, also can measure it performance and compare it with K-Means.

In terms of being able to help the Client understand their Customers' behaviors we can use the Hierarchical Clustering although we need another model to measure on performance and classify a customer as likely to generate Revenue or not


## Challenge the Solution
From the two Algorithm we can see it there were not sufficient in giving us performance, K-Means performed poorly while Hierarchical Clustering was able to form clusters but there is no way to measure performance

Therefore I believe we can explore on other Unsupervised Machine Learning Models like Neural Networks
